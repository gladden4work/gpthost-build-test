name: GPTHost Build Pipeline (Minimal)

on:
  workflow_dispatch:
    inputs:
      project_id:
        description: Project ID
        required: true
        type: string
      source_files:
        description: JSON object of files (path -> content)
        required: true
        type: string
      callback_url:
        description: Worker v2 callback URL
        required: true
        type: string
      callback_token:
        description: Bearer token for callback
        required: true
        type: string
      framework:
        description: react|vue|svelte|plain (optional)
        required: false
        type: string
      build_command:
        description: Custom build command (optional, ignored by minimal flow)
        required: false
        type: string
      build_config:
        description: Optional build config JSON (ignored by minimal flow)
        required: false
        type: string
      correlation_id:
        description: Correlation ID for tracing (optional)
        required: false
        type: string

permissions:
  contents: read

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Install tools (jq, awscli)
        run: |
          set -euo pipefail
          sudo apt-get update -y
          # Install jq and unzip from apt
          sudo apt-get install -y jq unzip
          # Install AWS CLI v2 from official source if not present
          if ! command -v aws >/dev/null 2>&1; then
            curl -fsSL https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip -o awscliv2.zip
            unzip -q awscliv2.zip
            sudo ./aws/install --update
          fi

      - name: Prepare workspace
        run: |
          set -euo pipefail
          mkdir -p "projects/${{ inputs.project_id }}/src"
          mkdir -p "projects/${{ inputs.project_id }}/public"
          mkdir -p "artifacts/${{ inputs.project_id }}"

      - name: Materialize source files
        working-directory: projects/${{ inputs.project_id }}
        env:
          SOURCE_FILES: ${{ inputs.source_files }}
        run: |
          set -euo pipefail
          echo "$SOURCE_FILES" > source.json
          jq -e . source.json >/dev/null
          jq -r 'to_entries[] | @base64' source.json | while read -r row; do
            key=$(echo "$row" | base64 --decode | jq -r '.key')
            content=$(echo "$row" | base64 --decode | jq -r '.value')
            key="${key#./}"
            case "$key" in
              *".."*) echo "Skipping unsafe path: $key" >&2; continue ;;
            esac
            case "$key" in
              public/*|src/*|index.html) target="$key" ;;
              *) target="src/$key" ;;
            esac
            mkdir -p "$(dirname "$target")"
            printf "%s" "$content" > "$target"
            echo "Created: $target"
          done
          if [ ! -f index.html ]; then
            printf '%s\n' \
              '<!DOCTYPE html>' \
              '<html lang="en">' \
              '  <head>' \
              '    <meta charset="UTF-8" />' \
              '    <meta name="viewport" content="width=device-width, initial-scale=1.0" />' \
              '    <title>GPTHost Minimal</title>' \
              '  </head>' \
              '  <body>' \
              '    <div id="root">GPTHost Minimal Build</div>' \
              '  </body>' \
              '</html>' > index.html
          fi

      - name: Minimal static build to dist/
        working-directory: projects/${{ inputs.project_id }}
        run: |
          set -euo pipefail
          rm -rf dist
          mkdir -p dist
          cp -f index.html dist/index.html
          if [ -d public ]; then cp -r public/. dist/; fi
          if [ -d src ]; then mkdir -p dist/src && cp -r src/. dist/src/; fi
          test -f dist/index.html

      - name: Prepare artifacts
        run: |
          set -euo pipefail
          cp -r "projects/${{ inputs.project_id }}/dist/." "artifacts/${{ inputs.project_id }}/"
          test -f "artifacts/${{ inputs.project_id }}/index.html"

      - name: Setup Python and boto3
        run: |
          pip install boto3
      
      - name: Upload dist to R2 (Python)
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          CF_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET_NAME }}
          PROJECT_ID: ${{ inputs.project_id }}
        run: |
          cat > upload-to-r2.py << 'EOF'
          #!/usr/bin/env python3
          import os
          import sys
          import subprocess
          
          # Get environment variables
          account_id = os.environ.get('CF_ACCOUNT_ID')
          access_key = os.environ.get('AWS_ACCESS_KEY_ID')
          secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
          bucket_name = os.environ.get('R2_BUCKET')
          project_id = os.environ.get('PROJECT_ID')
          
          if not all([account_id, access_key, secret_key, bucket_name, project_id]):
              print("Error: Missing required environment variables")
              sys.exit(1)
          
          # Construct endpoint URL
          endpoint_url = f"https://{account_id}.r2.cloudflarestorage.com"
          
          # Use AWS CLI with constructed endpoint
          source_path = f"artifacts/{project_id}/"
          dest_path = f"s3://{bucket_name}/builds/{project_id}/dist/"
          
          print(f"Uploading to R2...")
          print(f"Source: {source_path}")
          print(f"Destination: {dest_path}")
          
          cmd = [
              "aws", "s3", "cp", source_path, dest_path,
              "--recursive",
              "--endpoint-url", endpoint_url,
              "--no-progress"
          ]
          
          result = subprocess.run(cmd, capture_output=False, text=True)
          if result.returncode != 0:
              print(f"Upload failed with code {result.returncode}")
              sys.exit(1)
          
          print("Upload completed successfully")
          EOF
          
          python3 upload-to-r2.py

      - name: Success callback (auto-deploy)
        env:
          CALLBACK_URL: ${{ inputs.callback_url }}
          CALLBACK_TOKEN: ${{ inputs.callback_token }}
        run: |
          set -euo pipefail
          BODY=$(jq -n \
            --arg status "success" \
            --arg pid "${{ inputs.project_id }}" \
            --arg rid "${{ github.run_id }}" \
            --arg run_url "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" \
            --arg r2_path "builds/${{ inputs.project_id }}/dist/" \
            '{status:$status, project_id:$pid, github_run_id:$rid, github_run_url:$run_url, r2_build_path:$r2_path}')
          curl -sS -X POST "$CALLBACK_URL" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer $CALLBACK_TOKEN" \
            -d "$BODY"

      - name: Failure callback
        if: failure()
        env:
          CALLBACK_URL: ${{ inputs.callback_url }}
          CALLBACK_TOKEN: ${{ inputs.callback_token }}
        run: |
          set -euo pipefail
          BODY=$(jq -n \
            --arg status "failure" \
            --arg pid "${{ inputs.project_id }}" \
            --arg rid "${{ github.run_id }}" \
            --arg run_url "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" \
            '{status:$status, project_id:$pid, github_run_id:$rid, github_run_url:$run_url}')
          curl -sS -X POST "$CALLBACK_URL" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer $CALLBACK_TOKEN" \
            -d "$BODY"
