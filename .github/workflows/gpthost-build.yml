name: GPTHost Build Pipeline (Minimal)

on:
  workflow_dispatch:
    inputs:
      project_id:
        description: Project ID
        required: true
        type: string
      source_files:
        description: JSON object of files (path -> content)
        required: true
        type: string
      callback_url:
        description: Worker v2 callback URL
        required: true
        type: string
      callback_token:
        description: Bearer token for callback
        required: true
        type: string
      framework:
        description: react|vue|svelte|plain (optional)
        required: false
        type: string
      build_command:
        description: Custom build command (optional, ignored by minimal flow)
        required: false
        type: string
      build_config:
        description: Optional build config JSON (ignored by minimal flow)
        required: false
        type: string
      correlation_id:
        description: Correlation ID for tracing (optional)
        required: false
        type: string

permissions:
  contents: read

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Install tools (jq, awscli)
        run: |
          set -euo pipefail
          sudo apt-get update -y
          # Install jq and unzip from apt
          sudo apt-get install -y jq unzip
          # Install AWS CLI v2 from official source if not present
          if ! command -v aws >/dev/null 2>&1; then
            curl -fsSL https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip -o awscliv2.zip
            unzip -q awscliv2.zip
            sudo ./aws/install --update
          fi

      - name: Prepare workspace
        run: |
          set -euo pipefail
          mkdir -p "projects/${{ inputs.project_id }}/src"
          mkdir -p "projects/${{ inputs.project_id }}/public"
          mkdir -p "artifacts/${{ inputs.project_id }}"

      - name: Materialize source files
        working-directory: projects/${{ inputs.project_id }}
        env:
          SOURCE_FILES: ${{ inputs.source_files }}
        run: |
          set -euo pipefail
          echo "$SOURCE_FILES" > source.json
          jq -e . source.json >/dev/null
          jq -r 'to_entries[] | @base64' source.json | while read -r row; do
            key=$(echo "$row" | base64 --decode | jq -r '.key')
            content=$(echo "$row" | base64 --decode | jq -r '.value')
            key="${key#./}"
            case "$key" in
              *".."*) echo "Skipping unsafe path: $key" >&2; continue ;;
            esac
            case "$key" in
              public/*|src/*|index.html) target="$key" ;;
              *) target="src/$key" ;;
            esac
            mkdir -p "$(dirname "$target")"
            printf "%s" "$content" > "$target"
            echo "Created: $target"
          done
          if [ ! -f index.html ]; then
            printf '%s\n' \
              '<!DOCTYPE html>' \
              '<html lang="en">' \
              '  <head>' \
              '    <meta charset="UTF-8" />' \
              '    <meta name="viewport" content="width=device-width, initial-scale=1.0" />' \
              '    <title>GPTHost Minimal</title>' \
              '  </head>' \
              '  <body>' \
              '    <div id="root">GPTHost Minimal Build</div>' \
              '  </body>' \
              '</html>' > index.html
          fi

      - name: Minimal static build to dist/
        working-directory: projects/${{ inputs.project_id }}
        run: |
          set -euo pipefail
          rm -rf dist
          mkdir -p dist
          cp -f index.html dist/index.html
          if [ -d public ]; then cp -r public/. dist/; fi
          if [ -d src ]; then mkdir -p dist/src && cp -r src/. dist/src/; fi
          test -f dist/index.html

      - name: Prepare artifacts
        run: |
          set -euo pipefail
          cp -r "projects/${{ inputs.project_id }}/dist/." "artifacts/${{ inputs.project_id }}/"
          test -f "artifacts/${{ inputs.project_id }}/index.html"

      - name: Upload dist/ to Cloudflare R2 (builds/<project_id>/dist/)
        working-directory: artifacts/${{ inputs.project_id }}
        env:
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          set -euo pipefail
          # Sanitize potential stray whitespace from secrets
          ACCOUNT_ID_CLEAN=$(printf "%s" "$CLOUDFLARE_ACCOUNT_ID" | tr -d '\r' | tr -d '\n' | xargs)
          BUCKET_CLEAN=$(printf "%s" "$R2_BUCKET_NAME" | tr -d '\r' | tr -d '\n' | xargs)
          ENDPOINT="https://${ACCOUNT_ID_CLEAN}.r2.cloudflarestorage.com"
          export AWS_ACCESS_KEY_ID="$R2_ACCESS_KEY_ID"
          export AWS_SECRET_ACCESS_KEY="$R2_SECRET_ACCESS_KEY"
          export AWS_DEFAULT_REGION="us-east-1"
          export AWS_S3_FORCE_PATH_STYLE="true"
          export AWS_ENDPOINT_URL_S3="$ENDPOINT"
          export AWS_ENDPOINT_URL="$ENDPOINT"
          export AWS_REGION="us-east-1"
          aws --version || true
          echo "Using endpoint: $ENDPOINT"
          echo "Target bucket: $BUCKET_CLEAN"
          # Configure AWS CLI S3 client explicitly for R2 (config-layer)
          aws configure set default.s3.endpoint_url "$ENDPOINT"
          aws configure set default.s3.addressing_style path
          aws configure set default.region "$AWS_REGION"

          echo "Uploading files via s3api put-object..."
          DEST_PREFIX="builds/${{ inputs.project_id }}/dist/"
          # Find all files and upload individually using s3api (more reliable for custom endpoints)
          while IFS= read -r -d '' file; do
            key="${DEST_PREFIX}${file#./}"
            # Determine content type (best effort)
            ct="application/octet-stream"
            if command -v file >/dev/null 2>&1; then
              mt=$(file -b --mime-type "$file" || true)
              [ -n "${mt:-}" ] && ct="$mt"
            fi
            echo "Uploading $file -> s3://$BUCKET_CLEAN/$key ($ct)"
            aws s3api put-object \
              --bucket "$BUCKET_CLEAN" \
              --key "$key" \
              --body "$file" \
              --content-type "$ct" \
              --endpoint-url "$ENDPOINT" \
              --region "$AWS_REGION" >/dev/null
          done < <(find . -type f -print0)

      - name: Success callback (auto-deploy)
        env:
          CALLBACK_URL: ${{ inputs.callback_url }}
          CALLBACK_TOKEN: ${{ inputs.callback_token }}
        run: |
          set -euo pipefail
          BODY=$(jq -n \
            --arg status "success" \
            --arg pid "${{ inputs.project_id }}" \
            --arg rid "${{ github.run_id }}" \
            --arg run_url "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" \
            --arg r2_path "builds/${{ inputs.project_id }}/dist/" \
            '{status:$status, project_id:$pid, github_run_id:$rid, github_run_url:$run_url, r2_build_path:$r2_path}')
          curl -sS -X POST "$CALLBACK_URL" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer $CALLBACK_TOKEN" \
            -d "$BODY"

      - name: Failure callback
        if: failure()
        env:
          CALLBACK_URL: ${{ inputs.callback_url }}
          CALLBACK_TOKEN: ${{ inputs.callback_token }}
        run: |
          set -euo pipefail
          BODY=$(jq -n \
            --arg status "failure" \
            --arg pid "${{ inputs.project_id }}" \
            --arg rid "${{ github.run_id }}" \
            --arg run_url "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" \
            '{status:$status, project_id:$pid, github_run_id:$rid, github_run_url:$run_url}')
          curl -sS -X POST "$CALLBACK_URL" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer $CALLBACK_TOKEN" \
            -d "$BODY"
